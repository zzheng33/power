
## Scaling Up Modeling
performance modeling for distributed deep learning training

training samples in orignal dataset: M
traning samples in shrinked dataset: m
training epoches for orignal dataset: E
training epoches for shrinked dataset: e
training time taken for the orignal dataset for E epoch: T
training time taken for the shrinked dataset for e epoch: t

Suppose we train the m samples for e epoches on 1 Nvidia A100-40GB, the training time is t. 
Since the DL traning epoch is iterative and predictive, we can use the following analytic model to 
predict the total training time T, for the orignal dataset for E epoches. 

T = t * (M/m) * (E/e)


## Scaling Out Modeling
Now, we train the model in data parallel across multiple GPUs on the same node, 
and multiple GPUs on multiple nodes. 


GPU = 4
speedup = (T_1GPU)/(T_4gpu)
ideal_T = 2 
T_4gpu = 3
speedup = 8 / 3

efficiency = speedup / G = (8/3) / 4 = 2/3


