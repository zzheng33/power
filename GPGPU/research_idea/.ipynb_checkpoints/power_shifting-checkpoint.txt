Idea: 
shift power budge from CPU to GPU, and vice versa, based on their power demands. 

baseline: max_uncore and noshift
new approach: dynamic_uncore and shift

goal: remain same energy consumption while reducing run time


Conditions and Constraints:
1. Each compute node has a fixed power budget. 
2. Assume that CPU and GPU share the fixed power budget evenly across the execution by default. 


Previous research results:
1. Scaling the uncore frequency dynamically can save CPU power while maintain the lower performance loss. 


Goal: 
1. shift the power budget saved by the uncore frequency scaling to GPU to improve the performance. 



Prelinary Experiment Plan:

Cap the GPU power budget at for example 150W,
When the uncore frequency is set at min, shift the power budget contributed to 
decreasing uncore frequency to GPU, hence improve the GPU application with GPU-power hungry 
The challenge here is how to quantify the "reduced uncore frequency" into power in watts. 
Solution:
CPU power under steady state and max uncore frequency without any tasks: 176 W
CPU power under steady state and min uncore frequency without any tasks: 93 W
Hence, translate 1.4 GHz uncore frquency to watts: 83 W. 








########################

TDP for 2 sockets: 540 W
TDP for A100-40GB: 250 W

Suppose 60% Power cap for one node. 
2-sockets: 324 W
GPU: 150 W
